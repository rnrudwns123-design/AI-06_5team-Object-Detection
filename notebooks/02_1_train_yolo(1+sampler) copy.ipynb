{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "# ì‹œê°í™” ê´€ë ¨ ì„¤ì •\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'Apple SD Gothic Neo'\n",
    "except:\n",
    "    try:\n",
    "        plt.rcParams['font.family'] = 'NanumGothic'\n",
    "    except:\n",
    "        plt.rcParams['font.family'] = 'AppleGothic'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "fm._load_fontmanager(try_read_cache=False)\n",
    "\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\") # ë§¥ GPU\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\") # ìœˆë„ìš° GPU\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\") # CPU\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "# ìºì‹œ ì§€ìš°ê¸° í•¨ìˆ˜ ìƒì„±\n",
    "def clean_cache():\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# MallocStackLogging ì—ëŸ¬ ì¶œë ¥ ë°©ì§€\n",
    "os.environ.pop(\"MallocStackLogging\", None)\n",
    "os.environ.pop(\"MallocStackLoggingNoCompact\", None)\n",
    "os.environ.pop(\"DYLD_INSERT_LIBRARIES\", None)\n",
    "\n",
    "\n",
    "# # ë¡œê·¸\n",
    "# import logging\n",
    "\n",
    "# def init_logger() -> logging.Logger:\n",
    "#     logging.basicConfig(\n",
    "#         format=\"%(asctime)s [%(levelname)s] (%(filename)s:%(lineno)d) - %(message)s\",\n",
    "#         datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "#         level=logging.INFO,\n",
    "#         encoding=\"utf-8\",\n",
    "#     )\n",
    "#     return logging.getLogger(\"\")\n",
    "\n",
    "# logger = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, \"train_images\")\n",
    "ANNOT_DIR = os.path.join(DATA_DIR, \"train_annotations\")\n",
    "\n",
    "full_dict_path = os.path.join(DATA_DIR, \"FULL_DICT.json\")\n",
    "err_txt_path = os.path.join(DATA_DIR, \"err_image_paths.txt\")\n",
    "fixed_dict_path = os.path.join(DATA_DIR, \"FIXED_DICT.json\")\n",
    "\n",
    "FINAL_DICT = {}\n",
    "\n",
    "# FULL_DICT ë”í•˜ê¸°\n",
    "try:\n",
    "    with open(full_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        FULL_DICT = json.load(f)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for key, value in FULL_DICT.items():\n",
    "    a = os.path.join(IMAGE_DIR, key)\n",
    "    tmp_list = []\n",
    "    for pa in value:\n",
    "        tmp_list.append(os.path.join(ANNOT_DIR, pa))\n",
    "    FINAL_DICT[a] = tmp_list\n",
    "\n",
    "\n",
    "# err_image_paths ë¹¼ê¸°\n",
    "try:\n",
    "    with open(err_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        err_image_paths = f.read().split()\n",
    "    for err in err_image_paths:\n",
    "        a = os.path.join(IMAGE_DIR, err)\n",
    "        del FINAL_DICT[a]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# FIXED_DICT ë”í•˜ê¸°\n",
    "try:\n",
    "    with open(fixed_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        FIXED_DICT = json.load(f)\n",
    "    for key, value in FIXED_DICT:\n",
    "        a = os.path.join(IMAGE_DIR, key)\n",
    "        tmp_list = []\n",
    "        for pa in value:\n",
    "            tmp_list.append(os.path.join(ANNOT_DIR, pa))\n",
    "        FINAL_DICT[a] = tmp_list\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "for image_path, annot_paths in FINAL_DICT.items():\n",
    "\n",
    "    tmp_list = []\n",
    "\n",
    "    for path in annot_paths:\n",
    "        bbox = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        xywh_bbox = json_data[\"annotations\"][0][\"bbox\"]\n",
    "\n",
    "        tmp_list.append({\"bbox\": xywh_bbox,\n",
    "                        \"label\": json_data[\"categories\"][0][\"id\"]})\n",
    "        \n",
    "    FINAL_DICT[image_path] = tmp_list\n",
    "\n",
    "len(FINAL_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ í´ë˜ìŠ¤ ìˆ˜: 56\n",
      "í´ë˜ìŠ¤ ë§¤í•‘: {1899: 0, 2482: 1, 3350: 2, 3482: 3, 3543: 4, 3742: 5, 3831: 6, 4542: 7, 12080: 8, 12246: 9, 12777: 10, 13394: 11, 13899: 12, 16231: 13, 16261: 14, 16547: 15, 16550: 16, 16687: 17, 18146: 18, 18356: 19, 19231: 20, 19551: 21, 19606: 22, 19860: 23, 20013: 24, 20237: 25, 20876: 26, 21324: 27, 21770: 28, 22073: 29, 22346: 30, 22361: 31, 24849: 32, 25366: 33, 25437: 34, 25468: 35, 27732: 36, 27776: 37, 27925: 38, 27992: 39, 28762: 40, 29344: 41, 29450: 42, 29666: 43, 30307: 44, 31862: 45, 31884: 46, 32309: 47, 33008: 48, 33207: 49, 33879: 50, 34596: 51, 35205: 52, 36636: 53, 38161: 54, 41767: 55}\n",
      "ë‹¨ì¼ ìƒ˜í”Œ í´ë˜ìŠ¤(ë¬´ì¡°ê±´ Trainí–‰): 2ê°œ ì´ë¯¸ì§€\n",
      "\n",
      "ìµœì¢… ë¶„í•  ê²°ê³¼:\n",
      "Train ì´ë¯¸ì§€: 172ê°œ\n",
      "Val ì´ë¯¸ì§€: 43ê°œ\n",
      "\n",
      "ë¼ë²¨ íŒŒì¼ ìƒì„± ì¤‘...\n",
      "ì™„ë£Œ!\n",
      "\n",
      "YOLO ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ê²½ë¡œ: ./data/yolo_dataset\n",
      "data.yaml ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# YOLO ë°ì´í„°ì…‹ í´ë” ìƒì„±\n",
    "YOLO_BASE_PATH = \"./data/yolo_dataset\"\n",
    "os.makedirs(f\"{YOLO_BASE_PATH}/images/train\", exist_ok=True)\n",
    "os.makedirs(f\"{YOLO_BASE_PATH}/images/val\", exist_ok=True)\n",
    "os.makedirs(f\"{YOLO_BASE_PATH}/labels/train\", exist_ok=True)\n",
    "os.makedirs(f\"{YOLO_BASE_PATH}/labels/val\", exist_ok=True)\n",
    "\n",
    "# í´ë˜ìŠ¤ ID ë§¤í•‘ ìƒì„± (category id -> 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤)\n",
    "unique_labels = set()\n",
    "for annots in FINAL_DICT.values():\n",
    "    for annot in annots:\n",
    "        unique_labels.add(annot[\"label\"])\n",
    "\n",
    "label_to_idx = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "print(f\"ì´ í´ë˜ìŠ¤ ìˆ˜: {len(label_to_idx)}\")\n",
    "print(f\"í´ë˜ìŠ¤ ë§¤í•‘: {label_to_idx}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# [ìˆ˜ì •] ì¸µí™” ì¶”ì¶œ(Stratified Split)ì„ ìœ„í•œ ì „ëµ ìˆ˜ë¦½\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 1. ëª¨ë“  ë¼ë²¨ ìˆ˜ì§‘ ë° ë¹ˆë„ ê³„ì‚°\n",
    "all_labels = []\n",
    "image_paths = list(FINAL_DICT.keys())\n",
    "path_to_rarest_label = {} # ê° ì´ë¯¸ì§€ì˜ ëŒ€í‘œ(í¬ê·€) ë¼ë²¨ ì €ì¥\n",
    "\n",
    "# ì „ì²´ ë¹ˆë„ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°\n",
    "total_label_list = []\n",
    "for annots in FINAL_DICT.values():\n",
    "    for annot in annots:\n",
    "        total_label_list.append(annot[\"label\"])\n",
    "label_counts = Counter(total_label_list)\n",
    "\n",
    "# 2. ê° ì´ë¯¸ì§€ë³„ \"ê°€ì¥ í¬ê·€í•œ ë¼ë²¨\" ì„ ì •\n",
    "stratify_labels = []\n",
    "valid_image_paths = [] # ë¶„í• ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "for path in image_paths:\n",
    "    annots = FINAL_DICT[path]\n",
    "    if not annots:\n",
    "        # ë°°ê²½ ì´ë¯¸ì§€ëŠ” -1ë¡œ ì²˜ë¦¬\n",
    "        rarest_label = -1\n",
    "    else:\n",
    "        # ì´ë¯¸ì§€ ë‚´ ê°ì²´ ì¤‘ 'ì „ì²´ ë¹ˆë„ìˆ˜'ê°€ ê°€ì¥ ë‚®ì€ ê°ì²´ë¥¼ ì°¾ìŒ\n",
    "        # (ë¹ˆë„ìˆ˜, ë¼ë²¨) íŠœí”Œì„ ë§Œë“¤ì–´ minì„ ì°¾ìŒ\n",
    "        img_label_counts = [(label_counts[annot[\"label\"]], annot[\"label\"]) for annot in annots]\n",
    "        rarest_label_obj = min(img_label_counts, key=lambda x: x[0])\n",
    "        rarest_label = rarest_label_obj[1] # ë¼ë²¨ ê°’ë§Œ ì¶”ì¶œ\n",
    "\n",
    "    # ë§¤í•‘ëœ IDë¡œ ë³€í™˜ (ë°°ê²½ì€ -1 ìœ ì§€)\n",
    "    if rarest_label != -1:\n",
    "        mapped_id = label_to_idx[rarest_label]\n",
    "    else:\n",
    "        mapped_id = -1\n",
    "        \n",
    "    path_to_rarest_label[path] = mapped_id\n",
    "    valid_image_paths.append(path)\n",
    "    stratify_labels.append(mapped_id)\n",
    "\n",
    "# 3. [ì•ˆì „ì¥ì¹˜] ë°ì´í„° ê°œìˆ˜ê°€ 1ê°œë¿ì¸ í´ë˜ìŠ¤ ì²˜ë¦¬\n",
    "# stratify=y ì˜µì…˜ì€ yì˜ ê° í´ë˜ìŠ¤ ë°ì´í„°ê°€ ìµœì†Œ 2ê°œ ì´ìƒì´ì–´ì•¼ ì‘ë™í•¨ (í•˜ë‚˜ë¥¼ train, í•˜ë‚˜ë¥¼ valì— ë„£ì–´ì•¼ í•˜ë¯€ë¡œ)\n",
    "stratify_counter = Counter(stratify_labels)\n",
    "single_sample_classes = [lbl for lbl, cnt in stratify_counter.items() if cnt < 2]\n",
    "\n",
    "train_imgs = []\n",
    "val_imgs = []\n",
    "remaining_imgs = []\n",
    "remaining_labels = []\n",
    "\n",
    "for path, label in zip(valid_image_paths, stratify_labels):\n",
    "    if label in single_sample_classes:\n",
    "        # ë°ì´í„°ê°€ 1ê°œë¿ì¸ í´ë˜ìŠ¤ëŠ” ë¬´ì¡°ê±´ Trainì— ë„£ìŒ (Valì— ë„£ìœ¼ë©´ í•™ìŠµì„ ëª»í•˜ë¯€ë¡œ)\n",
    "        train_imgs.append(path)\n",
    "    else:\n",
    "        remaining_imgs.append(path)\n",
    "        remaining_labels.append(label)\n",
    "\n",
    "print(f\"ë‹¨ì¼ ìƒ˜í”Œ í´ë˜ìŠ¤(ë¬´ì¡°ê±´ Trainí–‰): {len(train_imgs)}ê°œ ì´ë¯¸ì§€\")\n",
    "\n",
    "# 4. ë‚¨ì€ ë°ì´í„°ë¡œ ì¸µí™” ì¶”ì¶œ ìˆ˜í–‰\n",
    "if remaining_imgs:\n",
    "    X_train, X_val, _, _ = train_test_split(\n",
    "        remaining_imgs, \n",
    "        remaining_labels, # yê°’ë§Œ í•„ìš”í•¨\n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=remaining_labels # í¬ê·€ ë¼ë²¨ ê¸°ì¤€ ë¹„ìœ¨ ìœ ì§€\n",
    "    )\n",
    "    \n",
    "    # ì•ì„œ ë¹¼ë‘” ë‹¨ì¼ ìƒ˜í”Œë“¤ê³¼ í•©ì¹˜ê¸°\n",
    "    train_images = train_imgs + X_train\n",
    "    val_images = val_imgs + X_val\n",
    "else:\n",
    "    # ë§Œì•½ ëª¨ë“  ë°ì´í„°ê°€ ë‹¨ì¼ ìƒ˜í”Œì´ë¼ë©´ (ê·¹ë‹¨ì  ê²½ìš°)\n",
    "    train_images = train_imgs\n",
    "    val_images = []\n",
    "\n",
    "print(f\"\\nìµœì¢… ë¶„í•  ê²°ê³¼:\")\n",
    "print(f\"Train ì´ë¯¸ì§€: {len(train_images)}ê°œ\")\n",
    "print(f\"Val ì´ë¯¸ì§€: {len(val_images)}ê°œ\")\n",
    "\n",
    "def convert_to_yolo_format(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    XYXY bboxë¥¼ YOLO format (normalized XYWH)ìœ¼ë¡œ ë³€í™˜\n",
    "    Args:\n",
    "        bbox: [x1, y1, x2, y2]\n",
    "        img_width, img_height: ì´ë¯¸ì§€ í¬ê¸°\n",
    "    Returns:\n",
    "        [x_center, y_center, width, height] (normalized)\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    \n",
    "    # ì¤‘ì‹¬ì ê³¼ ë„ˆë¹„/ë†’ì´ ê³„ì‚°\n",
    "    x_center = x + w/2\n",
    "    y_center = y + h/2\n",
    "    width = w\n",
    "    height = h\n",
    "    \n",
    "    # ì •ê·œí™” (0~1 ë²”ìœ„)\n",
    "    x_center /= img_width\n",
    "    y_center /= img_height\n",
    "    width /= img_width\n",
    "    height /= img_height\n",
    "    \n",
    "    return [x_center, y_center, width, height]\n",
    "\n",
    "def create_yolo_labels(image_paths, split='train'):\n",
    "    \"\"\"YOLO ë¼ë²¨ íŒŒì¼ ìƒì„± ë° ì´ë¯¸ì§€ ë³µì‚¬\"\"\"\n",
    "    for img_path in image_paths:\n",
    "        # ì´ë¯¸ì§€ í¬ê¸° ì½ê¸°\n",
    "        img = Image.open(img_path)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # ì´ë¯¸ì§€ íŒŒì¼ëª…\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        img_name = os.path.splitext(img_filename)[0]\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ë³µì‚¬\n",
    "        dst_img_path = f\"{YOLO_BASE_PATH}/images/{split}/{img_filename}\"\n",
    "        shutil.copy(img_path, dst_img_path)\n",
    "        \n",
    "        # ë¼ë²¨ íŒŒì¼ ìƒì„±\n",
    "        label_path = f\"{YOLO_BASE_PATH}/labels/{split}/{img_name}.txt\"\n",
    "        \n",
    "        with open(label_path, 'w') as f:\n",
    "            annots = FINAL_DICT[img_path]\n",
    "            for annot in annots:\n",
    "                # í´ë˜ìŠ¤ ID ë³€í™˜\n",
    "                class_id = label_to_idx[annot[\"label\"]]\n",
    "                \n",
    "                # bboxë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "                yolo_bbox = convert_to_yolo_format(\n",
    "                    annot[\"bbox\"], \n",
    "                    img_width, \n",
    "                    img_height\n",
    "                )\n",
    "                \n",
    "                # YOLO í˜•ì‹ìœ¼ë¡œ ì‘ì„±: <class> <x_center> <y_center> <width> <height>\n",
    "                f.write(f\"{class_id} {' '.join(map(str, yolo_bbox))}\\n\")\n",
    "\n",
    "# Train/Val ë¼ë²¨ ìƒì„±\n",
    "print(\"\\në¼ë²¨ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "create_yolo_labels(train_images, 'train')\n",
    "create_yolo_labels(val_images, 'val')\n",
    "print(\"ì™„ë£Œ!\")\n",
    "\n",
    "# YOLO data.yaml íŒŒì¼ ìƒì„±\n",
    "data_yaml = {\n",
    "    'path': os.path.abspath(YOLO_BASE_PATH),  # ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ\n",
    "    'train': 'images/train',  # train ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "    'val': 'images/val',      # val ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "    'nc': len(label_to_idx),  # í´ë˜ìŠ¤ ê°œìˆ˜\n",
    "    'names': idx_to_label     # í´ë˜ìŠ¤ ì´ë¦„ (idx: label)\n",
    "}\n",
    "\n",
    "import yaml\n",
    "with open(f\"{YOLO_BASE_PATH}/data.yaml\", 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "print(f\"\\nYOLO ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ê²½ë¡œ: {YOLO_BASE_PATH}\")\n",
    "print(f\"data.yaml ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics.data.build import InfiniteDataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "class CustomSamplerTrainer(DetectionTrainer):\n",
    "    def __init__(self, final_dict, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.final_dict = final_dict\n",
    "        \n",
    "    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\"):\n",
    "        if mode != \"train\":\n",
    "            return super().get_dataloader(dataset_path, batch_size, rank, mode)\n",
    "\n",
    "        with torch.distributed.distributed_c10d.monitored_barrier() if torch.distributed.is_initialized() else torch.no_grad():\n",
    "            dataset = self.build_dataset(dataset_path, mode, batch_size)\n",
    "        \n",
    "        # --- ê°€ì¤‘ì¹˜ ê³„ì‚° ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---\n",
    "        all_labels_in_dict = []\n",
    "        for info_list in self.final_dict.values():\n",
    "            for obj in info_list:\n",
    "                all_labels_in_dict.append(obj['label'])\n",
    "        \n",
    "        class_counts = Counter(all_labels_in_dict)\n",
    "        class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "        \n",
    "        filename_lookup = {os.path.basename(k): v for k, v in self.final_dict.items()}\n",
    "        sample_weights = []\n",
    "        \n",
    "        # YOLOê°€ ë¡œë“œí•œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ\n",
    "        for file_path in dataset.im_files:\n",
    "            filename = os.path.basename(file_path) # ê²½ë¡œ ë–¼ê³  íŒŒì¼ëª…ë§Œ ì¶”ì¶œ\n",
    "            \n",
    "            if filename in filename_lookup:\n",
    "                objects = filename_lookup[filename] # íŒŒì¼ëª…ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ\n",
    "                \n",
    "                if not objects:\n",
    "                    sample_weights.append(0.0)\n",
    "                    continue\n",
    "                \n",
    "                # ë¼ë²¨ ID ë§¤í•‘ ì£¼ì˜: \n",
    "                # FINAL_DICTì—ëŠ” ì›ë˜ ë¼ë²¨(ì˜ˆ: 16547)ì´ ë“¤ì–´ìˆê³ ,\n",
    "                # í•™ìŠµì—ëŠ” ë§¤í•‘ëœ ë¼ë²¨(ì˜ˆ: 0, 1)ì´ ì“°ì´ì§€ë§Œ,\n",
    "                # \"ë¹ˆë„ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜\"ë¥¼ ê³„ì‚°í•˜ëŠ” ë°ëŠ” ì›ë˜ ë¼ë²¨ì„ ì¨ë„ ìƒê´€ì—†ìŠµë‹ˆë‹¤.\n",
    "                # (í¬ê·€í•œ ê°ì²´ëŠ” ì›ë˜ IDë¡œë„ ì—¬ì „íˆ í¬ê·€í•˜ê¸° ë•Œë¬¸)\n",
    "                \n",
    "                current_img_weights = [class_weights.get(obj['label'], 0) for obj in objects]\n",
    "                sample_weights.append(max(current_img_weights))\n",
    "            else:\n",
    "                # ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ê²½ìš° (ê±°ì˜ ì—†ì–´ì•¼ ì •ìƒ)\n",
    "                print(f\"Warning: {filename} not found in lookup dict.\")\n",
    "                sample_weights.append(0.0)\n",
    "        if sum(sample_weights) == 0:\n",
    "            sample_weights = [1.0] * len(sample_weights)\n",
    "\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "        # 1. ì¼ë°˜ PyTorch DataLoader ìƒì„±\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(6148914691)\n",
    "        \n",
    "        loader = InfiniteDataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False, \n",
    "            sampler=sampler,\n",
    "            num_workers=self.args.workers,\n",
    "            collate_fn=getattr(dataset, 'collate_fn', None),\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=seed_worker, # ì‹œë“œ ê³ ì •ìš© (ì„ íƒì‚¬í•­)\n",
    "            generator=generator\n",
    "        )\n",
    "\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=None, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./data/yolo_dataset/data.yaml, degrees=5.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.0, exist_ok=False, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02, hsv_s=0.3, hsv_v=0.2, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=0.05, multi_scale=False, name=pill_y8s_512_aug1_light_with_sampler_refined, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.1, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=56\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2137720  ultralytics.nn.modules.head.Detect           [56, [128, 256, 512]]         \n",
      "Model summary: 129 layers, 11,157,272 parameters, 11,157,256 gradients, 28.8 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5168.4Â±4077.0 MB/s, size: 1753.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/data/yolo_dataset/labels/train... 207 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 207/207 1.8Kit/s 0.1s<0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/data/yolo_dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2097.6Â±143.2 MB/s, size: 1683.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/data/yolo_dataset/labels/val... 78 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 78/78 1.7Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/data/yolo_dataset/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/won/dev/codeit/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000167, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30      3.45G      1.492       5.11      1.384         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 21.5s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.8s/it 8.9s2.3ss\n",
      "                   all         78        261     0.0311      0.528      0.059     0.0508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30      3.45G     0.7649       3.04          1         28        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.4it/s 18.9s0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.7s/it 8.6s2.3ss\n",
      "                   all         78        261      0.237      0.421      0.263      0.229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30      3.45G     0.5928      2.226     0.9204         29        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.3it/s 19.5s0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.2s/it 11.1s.0ss\n",
      "                   all         78        261      0.592      0.583        0.6      0.524\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30      3.46G     0.5312      1.589     0.8949         26        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.3it/s 20.1s0.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.4s/it 12.2s3.3s\n",
      "                   all         78        261      0.715      0.683      0.747      0.644\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30      3.45G     0.4891      1.242     0.8674         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 21.2s0.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.3s/it 11.5s.1ss\n",
      "                   all         78        261      0.815      0.743      0.874      0.763\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30      3.46G     0.4708      1.071     0.8624         24        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 21.2s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 3.1s/it 15.3s4.3s\n",
      "                   all         78        261      0.852      0.837      0.939      0.821\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30      3.46G      0.455     0.9112     0.8511         31        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 22.0s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.9s/it 14.3s3.6ss\n",
      "                   all         78        261      0.844      0.919      0.972      0.883\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30      3.46G     0.4588     0.8328      0.861         26        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1it/s 23.1s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.9s/it 14.5s3.8ss\n",
      "                   all         78        261      0.829      0.948      0.983      0.844\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30      3.47G     0.4566     0.7409     0.8599         24        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 21.8s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 3.1s/it 15.7s4.3ss\n",
      "                   all         78        261      0.917      0.939      0.984      0.905\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30      3.47G     0.4673     0.6947     0.8516         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 22.0s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.3s/it 21.6s5.9ss\n",
      "                   all         78        261      0.952      0.967      0.993      0.843\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30      3.47G      0.445     0.6647      0.855         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 22.6s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 3.9s/it 19.5s5.5ss\n",
      "                   all         78        261       0.92      0.979      0.995      0.889\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/30      3.46G     0.4165     0.6241     0.8414         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 22.5s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.1s/it 20.5s5.6ss\n",
      "                   all         78        261      0.949       0.95      0.995      0.924\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/30      3.52G     0.4008     0.5583     0.8372         24        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2it/s 21.6s0.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.1s/it 20.6s5.5ss\n",
      "                   all         78        261      0.948      0.976      0.995      0.893\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/30      3.46G     0.3856     0.5442     0.8363         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1it/s 24.5s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 60% â”â”â”â”â”â”â”â”€â”€â”€â”€â”€ 3/5 5.7s/it 11.1s<11.4sWARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.1s/it 20.5s5.6s\n",
      "                   all         78        261      0.952      0.978      0.995      0.915\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/30      3.47G     0.3595     0.5507     0.8296         34        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.0s/it 26.3s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 60% â”â”â”â”â”â”â”â”€â”€â”€â”€â”€ 3/5 6.8s/it 13.8s<13.6sWARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 80% â”â”â”â”â”â”â”â”â”â•¸â”€â”€ 4/5 6.2s/it 18.9s<6.2sWARNING âš ï¸ NMS time limit 2.700s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.7s/it 23.7s\n",
      "                   all         78        261      0.958      0.944      0.961      0.901\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/30      3.46G     0.3477     0.5029      0.818         26        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1it/s 24.0s0.9ss\n",
      "WARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 40% â”â”â”â”â•¸â”€â”€â”€â”€â”€â”€â”€ 2/5 10.4s/it 10.6s<31.3sWARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 60% â”â”â”â”â”â”â”â”€â”€â”€â”€â”€ 3/5 8.5s/it 16.5s<17.0sWARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 80% â”â”â”â”â”â”â”â”â”â•¸â”€â”€ 4/5 7.2s/it 21.8s<7.2sWARNING âš ï¸ NMS time limit 2.700s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 5.4s/it 27.0s\n",
      "                   all         78        261      0.958      0.893      0.906      0.845\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/30      3.44G     0.3369     0.4889     0.8189         28        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1s/it 29.9s1.0ss\n",
      "WARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.6s/it 22.8s6.2ss\n",
      "                   all         78        261      0.962      0.979       0.99      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/30      3.44G     0.3413      0.482     0.8216         30        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.0s/it 26.4s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 60% â”â”â”â”â”â”â”â”€â”€â”€â”€â”€ 3/5 5.6s/it 12.1s<11.3sWARNING âš ï¸ NMS time limit 2.800s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 4.1s/it 20.4s5.6s\n",
      "                   all         78        261      0.957      0.991      0.995      0.955\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/30      3.44G     0.3218     0.4569       0.81         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1s/it 28.4s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 3.4s/it 17.0s4.2ss\n",
      "                   all         78        261      0.965      0.988      0.995       0.95\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/30      3.44G     0.3316     0.4327     0.8118         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.0s/it 26.2s0.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.9s/it 14.3s4.2s\n",
      "                   all         78        261      0.965      0.989      0.995      0.963\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/won/dev/codeit/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K      21/30      3.44G     0.3176     0.4102     0.8119         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.0it/s 25.9s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.0s/it 9.8s2.1ss\n",
      "                   all         78        261      0.959      0.988      0.995      0.962\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/30      3.44G     0.2962     0.4003     0.8007         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1s/it 27.6s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.0s/it 10.2s.7ss\n",
      "                   all         78        261      0.964      0.993      0.995      0.963\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/30      3.44G     0.2941     0.4068     0.7977         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2s/it 31.1s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.0s/it 9.9s2.4sss\n",
      "                   all         78        261       0.96      0.999      0.995       0.97\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/30      3.44G      0.302     0.4013      0.808         27        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.0it/s 25.9s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.6s/it 7.9s2.4ss\n",
      "                   all         78        261      0.973      0.995      0.995      0.959\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/30      3.44G     0.2874     0.3798     0.7977         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1s/it 27.4s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.6s/it 8.1s2.0ss\n",
      "                   all         78        261      0.966      0.992      0.995      0.977\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/30      3.44G     0.2749     0.3662     0.7958         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.3s/it 33.6s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.8s/it 9.1s2.5ss\n",
      "                   all         78        261      0.968      0.993      0.995      0.978\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/30      3.45G      0.282      0.364     0.8005         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1s/it 27.6s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.4s/it 7.1s2.0ss\n",
      "                   all         78        261       0.97      0.993      0.995      0.967\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/30      3.49G     0.2827     0.3595     0.8017         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.1s/it 27.9s1.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.5s/it 7.6s2.2ss\n",
      "                   all         78        261      0.973      0.997      0.995      0.968\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/30      3.44G     0.2764     0.3605     0.7944         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2s/it 30.2s1.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.5s/it 7.4s2.1ss\n",
      "                   all         78        261      0.975      0.996      0.995      0.977\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/30      3.44G     0.2836     0.3584     0.8018         25        512: 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 1.2s/it 32.4s1.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 1.2s/it 5.9s1.6ss\n",
      "                   all         78        261      0.976      0.997      0.995      0.978\n",
      "\n",
      "30 epochs completed in 0.333 hours.\n",
      "Optimizer stripped from /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/weights/best.pt...\n",
      "Ultralytics 8.3.229 ğŸš€ Python-3.10.19 torch-2.9.1 MPS (Apple M3)\n",
      "Model summary (fused): 72 layers, 11,147,256 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 2.7s/it 13.5s2.5s\n",
      "                   all         78        261      0.976      0.998      0.995      0.978\n",
      "                  1899          3          3       0.98          1      0.995      0.995\n",
      "                  2482          4          4      0.986          1      0.995      0.995\n",
      "                  3350         51         51      0.999          1      0.995      0.994\n",
      "                  3482         18         18      0.996          1      0.995      0.975\n",
      "                  3543          2          2      0.966          1      0.995      0.995\n",
      "                  3742          2          2      0.973          1      0.995      0.995\n",
      "                  3831          5          5      0.981          1      0.995      0.941\n",
      "                  4542          2          2       0.96          1      0.995      0.995\n",
      "                 12080          1          1       0.95          1      0.995      0.995\n",
      "                 12777          3          3      0.973          1      0.995      0.995\n",
      "                 13394          2          2      0.969          1      0.995      0.995\n",
      "                 13899          4          4      0.991          1      0.995      0.971\n",
      "                 16231          6          6      0.983          1      0.995      0.995\n",
      "                 16261         11         11      0.995          1      0.995      0.995\n",
      "                 16547          4          4      0.982          1      0.995      0.964\n",
      "                 16550          1          1      0.945          1      0.995      0.995\n",
      "                 16687          2          2      0.971          1      0.995      0.921\n",
      "                 18146          7          7          1      0.878      0.995      0.918\n",
      "                 18356          4          4       0.99          1      0.995      0.995\n",
      "                 19231          6          6      0.988          1      0.995      0.995\n",
      "                 19551          1          1      0.962          1      0.995      0.995\n",
      "                 19860          3          3      0.979          1      0.995      0.995\n",
      "                 20013          4          4      0.983          1      0.995      0.995\n",
      "                 20237          8          8      0.989          1      0.995      0.995\n",
      "                 20876          6          6      0.986          1      0.995      0.995\n",
      "                 21324          6          6          1      0.992      0.995      0.968\n",
      "                 21770          1          1      0.954          1      0.995      0.995\n",
      "                 22073          6          6      0.999          1      0.995       0.96\n",
      "                 22346          3          3      0.972          1      0.995      0.995\n",
      "                 22361          1          1      0.943          1      0.995      0.995\n",
      "                 24849          1          1       0.94          1      0.995      0.895\n",
      "                 25366          4          4      0.993          1      0.995      0.995\n",
      "                 25437          2          2      0.961          1      0.995      0.995\n",
      "                 25468          3          3      0.976          1      0.995      0.995\n",
      "                 27732          5          5      0.979          1      0.995      0.915\n",
      "                 27776          1          1      0.955          1      0.995      0.995\n",
      "                 27925          1          1      0.947          1      0.995      0.995\n",
      "                 27992          2          2      0.963          1      0.995      0.921\n",
      "                 28762          5          5      0.987          1      0.995      0.995\n",
      "                 29344          2          2      0.962          1      0.995      0.921\n",
      "                 29666          8          8      0.989          1      0.995      0.974\n",
      "                 30307          2          2      0.969          1      0.995      0.995\n",
      "                 31862          5          5      0.984          1      0.995      0.995\n",
      "                 31884          2          2      0.973          1      0.995      0.995\n",
      "                 32309          6          6      0.991          1      0.995      0.995\n",
      "                 33207          1          1       0.97          1      0.995      0.995\n",
      "                 33879          5          5      0.983          1      0.995      0.961\n",
      "                 34596          2          2      0.963          1      0.995      0.947\n",
      "                 35205          7          7      0.991          1      0.995      0.961\n",
      "                 36636          8          8      0.994          1      0.995      0.979\n",
      "                 38161          7          7      0.988          1      0.995       0.96\n",
      "                 41767          5          5      0.972          1      0.995      0.995\n",
      "Speed: 0.2ms preprocess, 40.7ms inference, 0.0ms loss, 16.8ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLO ëª¨ë¸ ë¡œë“œ (ì‚¬ì „í•™ìŠµëœ ëª¨ë¸)\n",
    "\n",
    "model_name = 'yolov8s.pt' # í˜¹ì€ ê°€ì§€ê³  ê³„ì‹  ê°€ì¤‘ì¹˜ ê²½ë¡œ\n",
    "\n",
    "args = dict(\n",
    "    model=model_name,\n",
    "    data='./data/yolo_dataset/data.yaml',\n",
    "    epochs=30,\n",
    "    imgsz=512,\n",
    "    batch=8,\n",
    "    name='pill_y8s_512_aug1_light_with_sampler_refined',\n",
    "    device=DEVICE,  # 'mps', 'cuda', 'cpu'\n",
    "\n",
    "    # ğŸ‘‡ ì—¬ê¸°ë¶€í„° ì¦ê°• ì„¸íŒ…\n",
    "    # ê¸°í•˜í•™ ë³€í˜•\n",
    "    degrees=5.0,        # Â±5ë„ íšŒì „ (ì´¬ì˜ ê°ë„ ê°œì¸ì°¨)\n",
    "    translate=0.05,     # 5% ì •ë„ í‰í–‰ ì´ë™\n",
    "    scale=0.10,         # 10% í™•ëŒ€/ì¶•ì†Œ\n",
    "    shear=0.0,          # ê°ì¸ ì°Œê·¸ëŸ¬ì§ ë°©ì§€\n",
    "    perspective=0.0,    # ì›ê·¼ ì™œê³¡ X\n",
    "    fliplr=0.0,         # ì¢Œìš°ë°˜ì „ X (ê°ì¸ ë°©í–¥ ë³´ì¡´)\n",
    "    flipud=0.0,         # ìƒí•˜ë°˜ì „ X\n",
    "\n",
    "    # ìƒ‰/ë°ê¸° (ColorJitter + BrightnessContrast ëŠë‚Œ)\n",
    "    hsv_h=0.02,         # ìƒ‰ì¡° ì‚´ì§ ë³€í™”\n",
    "    hsv_s=0.30,         # ì±„ë„ ì ë‹¹íˆ\n",
    "    hsv_v=0.20,         # ë°ê¸° ì ë‹¹íˆ\n",
    "\n",
    "    # ëª¨ìì´í¬/ë¯¹ìŠ¤ì—… ë“±\n",
    "    mosaic=0.05,        # ì‚´ì§ë§Œ ì‚¬ìš© (ë°ì´í„° ì ì–´ì„œ ê³¼í•˜ë©´ ìœ„í—˜)\n",
    "    mixup=0.0,\n",
    "    erasing=0.0,\n",
    "    auto_augment=None,\n",
    "    workers=2\n",
    ")\n",
    "\n",
    "# 3. ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆë¡œ í•™ìŠµ ì‹œì‘\n",
    "trainer = CustomSamplerTrainer(final_dict=FINAL_DICT, overrides=args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ìµœì‹  train run ì„ íƒ: ./runs/detect/pill_y8s_512_aug1_light_with_sampler_refined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ í‘œì‹œ\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def show_latest_training_results():\n",
    "    \"\"\"\n",
    "    ./runs/detect ì•ˆì—ì„œ 'predict', 'val'ì„ ì œì™¸í•œ\n",
    "    ê°€ì¥ ìµœê·¼ train runì„ ê³ ë¥´ê³ ,\n",
    "    metric ì´ë¯¸ì§€ë¥¼ 2ê°œì”© í•œ ì¤„ì— ì‹œê°í™”.\n",
    "    \"\"\"\n",
    "    # 1) train run í´ë”ë§Œ ëª¨ìœ¼ê¸°\n",
    "    run_dirs = []\n",
    "    for d in glob.glob('./runs/detect/*'):\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        name = os.path.basename(d).lower()\n",
    "        if 'predict' in name or name == 'val':\n",
    "            continue\n",
    "        run_dirs.append(d)\n",
    "\n",
    "    if not run_dirs:\n",
    "        print(\"âš ï¸ í•™ìŠµ(run) í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 2) ê°€ì¥ ìµœê·¼ train run ì„ íƒ\n",
    "    latest_run = sorted(run_dirs, key=os.path.getmtime)[-1]\n",
    "    print(f\"ğŸ“ ìµœì‹  train run ì„ íƒ: {latest_run}\")\n",
    "\n",
    "    # 3) ì‚¬ìš©í•  metric ì´ë¯¸ì§€ í›„ë³´\n",
    "    candidates = [\n",
    "        'results.png',\n",
    "        'confusion_matrix.png',\n",
    "        'confusion_matrix_normalized.png',\n",
    "        'BoxF1_curve.png',\n",
    "        'BoxP_curve.png',\n",
    "        'BoxR_curve.png',\n",
    "    ]\n",
    "    metric_images = [img for img in candidates\n",
    "                     if os.path.exists(os.path.join(latest_run, img))]\n",
    "\n",
    "    if not metric_images:\n",
    "        print(\"âš ï¸ í‘œì‹œí•  metric ì´ë¯¸ì§€ê°€ ì—†ì–´ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 4) 2ê°œì”© í•œ ì¤„ì— ë°°ì¹˜\n",
    "    n = len(metric_images)\n",
    "    cols = 2\n",
    "    rows = (n + cols - 1) // cols  # ì˜¬ë¦¼\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(14, 6 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)  # (2,) -> (1,2)\n",
    "\n",
    "    fig.suptitle(f'YOLO Training Results\\n{latest_run}',\n",
    "                 fontsize=18, fontweight='bold')\n",
    "\n",
    "    for idx, img_name in enumerate(metric_images):\n",
    "        r = idx // cols\n",
    "        c = idx % cols\n",
    "        ax = axes[r, c]\n",
    "\n",
    "        img_path = os.path.join(latest_run, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(img_name.replace('.png', '').replace('_', ' ').title())\n",
    "        ax.axis('off')\n",
    "\n",
    "    # ë‚¨ëŠ” ì¹¸ì€ ìˆ¨ê¸°ê¸°\n",
    "    for idx in range(len(metric_images), rows * cols):\n",
    "        r = idx // cols\n",
    "        c = idx % cols\n",
    "        axes[r, c].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "show_latest_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. results.csv ì½ê¸° (ê²½ë¡œ ìˆ˜ì • í•„ìš”)\n",
    "df = pd.read_csv('runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/results.csv')\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ê³µë°± ì œê±° (YOLO csvëŠ” ì»¬ëŸ¼ëª… ì•ì— ê³µë°±ì´ ìˆì„ ìˆ˜ ìˆìŒ)\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "# 2. ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ì˜ˆ: mAP50-95 ì™€ train/box_loss)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# mAP@50-95 (ì „ì²´ì ì¸ ì„±ëŠ¥ ì§€í‘œ)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@50-95')\n",
    "plt.title('Validation mAP Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mAP')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Train Box Loss (í•™ìŠµì´ ì˜ ë˜ê³  ìˆëŠ”ì§€)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['train/box_loss'], label='Train Box Loss', color='orange')\n",
    "plt.plot(df['epoch'], df['val/box_loss'], label='Val Box Loss', color='red')\n",
    "plt.title('Box Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/data/train_images/K-003483-020877-031885-035206_0_2_0_2_90_000_200.png: 512x416 1 3482, 1 20876, 1 31884, 1 35205, 47.7ms\n",
      "Speed: 2.0ms preprocess, 47.7ms inference, 5.0ms postprocess per image at shape (1, 3, 512, 416)\n",
      "Results saved to \u001b[1m/Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/predict5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ì •ë³´ ===\n",
      "ê°ì²´: 3482 | ì‹ ë¢°ë„: 0.99 | ì¢Œí‘œ: (648.1, 745.8, 861.2, 1018.9)\n",
      "ê°ì²´: 35205 | ì‹ ë¢°ë„: 0.98 | ì¢Œí‘œ: (648.5, 172.8, 878.8, 577.4)\n",
      "ê°ì²´: 20876 | ì‹ ë¢°ë„: 0.98 | ì¢Œí‘œ: (114.6, 766.2, 448.2, 1001.9)\n",
      "ê°ì²´: 31884 | ì‹ ë¢°ë„: 0.97 | ì¢Œí‘œ: (117.8, 41.6, 540.9, 556.9)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 1. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ê²½ë¡œëŠ” ë³¸ì¸ì˜ runs í´ë” í™•ì¸ í•„ìš”)\n",
    "# ë³´í†µ 'runs/detect/train/weights/best.pt' ì— ì €ì¥ë©ë‹ˆë‹¤.\n",
    "model = YOLO('runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/weights/best.pt')\n",
    "\n",
    "rand_index = random.randint(1, len(glob.glob(os.path.join(IMAGE_DIR, \"*.png\"))))\n",
    "\n",
    "# 2. ì˜ˆì¸¡í•  ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "# (í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”)\n",
    "source_img_path = glob.glob(os.path.join(IMAGE_DIR, \"*.png\"))[rand_index-1]\n",
    "\n",
    "# 3. ì˜ˆì¸¡ ì‹¤í–‰ (conf: í™•ì‹  ì„ê³„ê°’, save: ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€)\n",
    "results = model.predict(source_img_path, conf=0.5, save=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [ì‹œê°í™”] ì˜ˆì¸¡ ê²°ê³¼ ì´ë¯¸ì§€ í™”ë©´ì— ë„ìš°ê¸°\n",
    "# ---------------------------------------------------------\n",
    "for result in results:\n",
    "    # result.plot()ì€ BGR(Opencv í¬ë§·)ë¡œ ëœ numpy arrayë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    plotted_img = result.plot()\n",
    "    \n",
    "    # Matplotlib ì¶œë ¥ì„ ìœ„í•´ BGR -> RGB ë³€í™˜\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(plotted_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [í…ìŠ¤íŠ¸ ì¶œë ¥] ì¢Œí‘œ, í´ë˜ìŠ¤, ì •í™•ë„ ì¶œë ¥\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n=== ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ì •ë³´ ===\")\n",
    "for result in results:\n",
    "    boxes = result.boxes  # ê°ì§€ëœ ë°•ìŠ¤ë“¤\n",
    "    \n",
    "    for box in boxes:\n",
    "        # 1. ì¢Œí‘œ (x1, y1, x2, y2)\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        \n",
    "        # 2. í´ë˜ìŠ¤ ì´ë¦„\n",
    "        cls_id = int(box.cls[0])\n",
    "        cls_name = model.names[cls_id]\n",
    "        \n",
    "        # 3. ì‹ ë¢°ë„ (Confidence Score)\n",
    "        conf = float(box.conf[0])\n",
    "        \n",
    "        print(f\"ê°ì²´: {cls_name} | ì‹ ë¢°ë„: {conf:.2f} | ì¢Œí‘œ: ({x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR : /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection\n",
      "TEST_DIR : /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/data/test_images\n",
      "MODEL_PATH : /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/runs/detect/pill_y8s_512_aug1_light_with_sampler_refined/weights/best.pt\n",
      "DATA_YAML_PATH : /Users/won/dev/codeit/0_mission/100_DL_ObjectDetection/notebooks/data/yolo_dataset/data.yaml\n",
      "   annotation_id  image_id  category_id      bbox_x      bbox_y      bbox_w  \\\n",
      "0              1         1         1899  155.912247  250.397949  204.381943   \n",
      "1              2         1        27925  598.417603  673.536194  263.552368   \n",
      "2              3         1        16550  555.981506   64.747505  395.370178   \n",
      "3              4         1        38161  172.553253  743.142395  186.509003   \n",
      "4              5         3         1899  139.319183  239.985916  198.302887   \n",
      "\n",
      "       bbox_h     score  \n",
      "0  127.148438  0.993433  \n",
      "1  480.769714  0.985108  \n",
      "2  416.079979  0.982620  \n",
      "3  288.507019  0.542624  \n",
      "4  129.416306  0.993691  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())          # notebooks ê¸°ì¤€ í•œ ë‹¨ê³„ ìœ„\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test_images\") # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë” ì´ë¦„ í™•ì¸!\n",
    "MODEL_PATH = os.path.join(os.getcwd(), \"runs\", \"detect\", \"pill_y8s_512_aug1_light_with_sampler_refined\", \"weights\", \"best.pt\")\n",
    "DATA_YAML_PATH = os.path.join(os.getcwd(), \"data\", \"yolo_dataset\", \"data.yaml\")\n",
    "print(\"ROOT_DIR :\", ROOT_DIR)\n",
    "print(\"TEST_DIR :\", TEST_DIR)\n",
    "print(\"MODEL_PATH :\", MODEL_PATH)\n",
    "print(\"DATA_YAML_PATH :\", DATA_YAML_PATH)\n",
    "\n",
    "# 2. data.yamlì—ì„œ idx -> category_id\n",
    "with open(DATA_YAML_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "names = data_cfg[\"names\"]   # {0: 1899, 1: 2482, ...}[file:24]\n",
    "\n",
    "# 3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ëª©ë¡ (íŒŒì¼ ì´ë¦„ ê¸°ì¤€)\n",
    "test_image_paths = glob.glob(os.path.join(TEST_DIR, \"*.png\"))\n",
    "\n",
    "# íŒŒì¼ëª…ì—ì„œ ìˆ«ìë§Œ ë½‘ì•„ì„œ ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©\n",
    "test_image_paths = sorted(\n",
    "    test_image_paths,\n",
    "    key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))\n",
    "\n",
    "# 4. ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡\n",
    "model = YOLO(MODEL_PATH)\n",
    "results = model(test_image_paths, conf=0.25, iou=0.5, verbose=False)\n",
    "\n",
    "# 5. ê²°ê³¼ â†’ submission rows\n",
    "rows = []\n",
    "ann_id = 1\n",
    "\n",
    "for img_path, res in zip(test_image_paths, results):\n",
    "    fname = os.path.basename(img_path)         # \"3.png\", \"10.png\" ê·¸ëŒ€ë¡œ\n",
    "    image_id = int(os.path.splitext(fname)[0]) # \"3\" -> 3, \"10\" -> 10\n",
    "    ...\n",
    "    # 1, 3, 4, ...\n",
    "\n",
    "    if res.boxes is None or len(res.boxes) == 0:\n",
    "        # ë°•ìŠ¤ ì—†ëŠ” ì´ë¯¸ì§€ëŠ” ê·œì¹™ì— ë”°ë¼ ì²˜ë¦¬ (ë³´í†µ ì•„ë¬´ í–‰ë„ ì•ˆ ë„£ì–´ë„ ë¨)\n",
    "        continue\n",
    "\n",
    "    for box in res.boxes:\n",
    "        cls_idx = int(box.cls.item())                   # YOLO class index\n",
    "        category_id = int(names[cls_idx])               # ì‹¤ì œ category_id ìˆ«ì[file:24]\n",
    "\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        score = float(box.conf.item())\n",
    "\n",
    "        rows.append({\n",
    "            \"annotation_id\": ann_id,\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category_id,\n",
    "            \"bbox_x\": x1,\n",
    "            \"bbox_y\": y1,\n",
    "            \"bbox_w\": w,\n",
    "            \"bbox_h\": h,\n",
    "            \"score\": score,\n",
    "        })\n",
    "        ann_id += 1\n",
    "\n",
    "# 6. CSVë¡œ ì €ì¥\n",
    "sub = pd.DataFrame(rows, columns=[\n",
    "    \"annotation_id\", \"image_id\", \"category_id\",\n",
    "    \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"\n",
    "])\n",
    "print(sub.head())\n",
    "sub.to_csv(os.path.join(ROOT_DIR, \"submission_test_fixed.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "codeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
